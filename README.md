# Statistical Data Analysis 2

This is the official repository containing Lab materials for Statistical Data Analysis 2 (SAD2) course. 

Expect lab materials in `.ipynb` or `.pdf` format to appear before the class begins and solutions for the programming exercises before the start of the next lab, a week later. 

We will also post here additional materials mentioned during lab meetings. 

We encourage you to connect, collaborate and work together on Lab materials and lectures on [Discord](https://discord.gg/DYE7YZUN). We are always open and happy to discuss topics related to the course, so please feel free to reach out and ask questions on Discord.

## Topics

- 0: Useful across all labs
    - [Matplotlib visualisations for pyro and torch.distributions](http://jonathanpchen.com/makeplot/)
    - [Good **research** code](https://goodresearch.dev/_static/book.pdf)

- Lab 1: Frequentist vs Bayesian: Intro to torch.distributions + Likelihoods
    - [Reasoning about Shapes and Probability Distributions](https://ericmjl.github.io/blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/) 
    - [When is MLE equal to MAP?](https://agustinus.kristia.de/techblog/2017/01/01/mle-vs-map/)
    - [What is the connection between MLE and MAP?](https://stats.stackexchange.com/questions/235070/relation-between-map-em-and-mle)
    - [What is the connection between MLE and MAP II?](https://math.stackexchange.com/questions/2917109/map-solution-for-linear-regression-what-is-a-gaussian-prior)
    - [What is likelihood, really?](https://stats.stackexchange.com/questions/2641/what-is-the-difference-between-likelihood-and-probability)
    - Bootstrap - Chapter 8 in Wasserman \[4\] 
    - [What is the difference between maximum likelihood estimation (MLE) vs. least squares estimaton (LSE)?](https://stats.stackexchange.com/questions/143705/maximum-likelihood-method-vs-least-squares-method)
    - ["Learning probabilistic models" lecture by Roger Grosse and Nitish Srivastava (especially 3.3)](http://www.cs.toronto.edu/~rgrosse/csc321/probabilistic_models.pdf)

- Lab 2: Bayesian Nets: conditional independence, Markov's blanket, Berkson's paradox, d-separation 
    - [Berkson paradox in medical care](https://onlinelibrary.wiley.com/doi/full/10.1111/joim.12363)
    - [Does Hollywood ruin books? - Numberphile](https://www.youtube.com/watch?v=FUD8h9JpEVQ&ab_channel=Numberphile)
    - [The selection-distortion effect: How selection changes correlations in surprising ways: The burger-fries tradeoff](https://thehardestscience.com/2014/08/04/the-selection-distortion-effect-how-selection-changes-correlations-in-surprising-ways/)
    - [Berkson's paradox with lots of examples on Brilliant](https://brilliant.org/wiki/berksons-paradox/) 
    - [d-separation WITHOUT TEARS](http://bayes.cs.ucla.edu/BOOK-2K/d-sep.html)
    - [Statistical Rethinking 2022 on Confounders](https://www.youtube.com/watch?v=UpP-_mBvECI&ab_channel=RichardMcElreath)


- Lab 3: Gaussian Mixture Models & Expectation Maximization
    - [Broad overview of EM by Karin Knudson](https://karinknudson.com/expectationmaximization.html)
    - [What is the difference between EM and gradient descent?](https://stats.stackexchange.com/questions/45652/what-is-the-difference-between-em-and-gradient-ascent/45653#45653)
    - [When k-means algorithm corresponds to EM?](https://perso.telecom-paristech.fr/bonald/documents/gmm.pdf)
    - ["Mixture models" lecture by Roger Grosse and Nitish Srivastava](https://www.cs.toronto.edu/~rgrosse/csc321/mixture_models.pdf)


## Additional Materials 

- Notes created by students and curated by us: [link](https://www.overleaf.com/1932227257jjpwpnrcwmjj)
- Who works on which topic: [link](https://docs.google.com/spreadsheets/d/1y92labArnVnMzc2nIDPOBBZA0gcn_Ep9ZEN9kvTEfXk/edit#gid=1392012547)
- Annotations: [link](https://www.zotero.org/groups/4805090/sad2/library)

## Cool Papers and Books to read that we don't have time to talk about more (good starting points for projects etc.) 

- [Leo Breiman. Statistical Modeling: The Two Cultures](https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.full)
- [Edwin Thompson Jaynes. Probability Theory: The Logic of Science, Chapter 1](https://bayes.wustl.edu/etj/prob/book.pdf)

## References
1. [Christopher M. Bishop. Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)
2. [David JC MacKay, et al. Information theory, inference and learning
algorithms.](https://www.inference.org.uk/itprnn/book.pdf)
3. [B. Lambert. A Studentâ€™s Guide to Bayesian Statistics.](https://ben-lambert.com/a-students-guide-to-bayesian-statistics/)
4. [Larry A. Wasserman. All of Statistics: A Concise Course in Statistical Inference](https://egrcc.github.io/docs/math/all-of-statistics.pdf)

## Prerequisites
TBD

