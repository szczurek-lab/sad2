{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "au8A6Jmh5BKy"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "try:\n",
        "  import pyro \n",
        "except ModuleNotFoundError:\n",
        "  !pip install pyro-ppl\n",
        "  import pyro\n"
      ],
      "metadata": {
        "id": "dKTKOJCxqgUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensors and PyTorch\n",
        "\n",
        "Source: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial2/Introduction_to_PyTorch.html"
      ],
      "metadata": {
        "id": "au8A6Jmh5BKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basics"
      ],
      "metadata": {
        "id": "jipFhODL6UzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start with reviewing the basic concepts of PyTorch. As a prerequisite, we recommend to be familiar with the *numpy* package as most machine learning frameworks are based on very similar concepts. If you are not familiar with *numpy* yet, don't worry: here is a tutorial to go through. If you are familiar though, you might want to skip this.\n",
        "\n",
        "Let's start with importing PyTorch. The package is called torch, based on its original framework Torch. As a first step, we can check its version:"
      ],
      "metadata": {
        "id": "tAaaJ30a5a-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(\"Using torch\", torch.__version__)"
      ],
      "metadata": {
        "id": "MuxTGyp65FGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As in every machine learning framework, PyTorch provides functions that are stochastic like generating random numbers. However, a very good practice is to setup your code to be reproducible with the exact same random numbers. This is why we set a seed below. As everyone knows, 42 is the best seed. "
      ],
      "metadata": {
        "id": "kenLf3Q55UYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42) # Setting the seed"
      ],
      "metadata": {
        "id": "jN4YeYK85O92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensors"
      ],
      "metadata": {
        "id": "UtMeAk5f6aIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Letâ€™s first start by looking at different ways of creating a tensor. There are many possible options, the simplest one is to call torch.Tensor passing the desired shape as input argument:"
      ],
      "metadata": {
        "id": "tjiKlKtK5kMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.Tensor(2, 3, 4)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "13JCypzX5rb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `torch.Tensor` allocates memory for the desired tensor, but reuses any values that have already been in the memory. To directly assign values to the tensor during initialization, there are many alternatives including:\n",
        "\n",
        "* `torch.zeros`: Creates a tensor filled with zeros\n",
        "* `torch.ones`: Creates a tensor filled with ones\n",
        "* `torch.rand`: Creates a tensor with random values uniformly sampled between 0 and 1\n",
        "* `torch.randn`: Creates a tensor with random values sampled from a normal distribution with mean 0 and variance 1\n",
        "* `torch.arange`: Creates a tensor containing the values $N,N+1,N+2,...,M$\n",
        "* `torch.Tensor` (input list): Creates a tensor from the list elements you provide"
      ],
      "metadata": {
        "id": "au3FWPn85zhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor from a (nested) list\n",
        "x = torch.Tensor([[1, 2], [3, 4]])\n",
        "print(x)"
      ],
      "metadata": {
        "id": "puiZq2LR5xaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor with random values between 0 and 1 with the shape [2, 3, 4]\n",
        "x = torch.rand(2, 3, 4)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "dBTpJxZQ52kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can obtain the shape of a tensor in the same way as in numpy (x.shape), or using the .size method:\n",
        "\n"
      ],
      "metadata": {
        "id": "LKgVwjKJ55Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shape = x.shape\n",
        "print(\"Shape:\", x.shape)\n",
        "\n",
        "size = x.size()\n",
        "print(\"Size:\", size)\n",
        "\n",
        "dim1, dim2, dim3 = x.size()\n",
        "print(\"Size:\", dim1, dim2, dim3)"
      ],
      "metadata": {
        "id": "aDtIG6AX558M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensors and NumPy"
      ],
      "metadata": {
        "id": "JrN2UZAb6haX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensors can be converted to numpy arrays, and numpy arrays back to tensors. To transform a numpy array into a tensor, we can use the function torch.from_numpy:"
      ],
      "metadata": {
        "id": "S0dGF9-Z57-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np_arr = np.array([[1, 2], [3, 4]])\n",
        "tensor = torch.from_numpy(np_arr)\n",
        "\n",
        "print(\"Numpy array:\", np_arr)\n",
        "print(\"PyTorch tensor:\", tensor)"
      ],
      "metadata": {
        "id": "TodvhshX5-AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To transform a PyTorch tensor back to a numpy array, we can use the function .numpy() on tensors:"
      ],
      "metadata": {
        "id": "nkAc9l1s6AVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.arange(4)\n",
        "np_arr = tensor.numpy()\n",
        "\n",
        "print(\"PyTorch tensor:\", tensor)\n",
        "print(\"Numpy array:\", np_arr)"
      ],
      "metadata": {
        "id": "sD0D4aBD6CYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The conversion of tensors to numpy require the tensor to be on the CPU, and not the GPU (more on GPU support in a later section). In case you have a tensor on GPU, you need to call `.cpu()` on the tensor beforehand. Hence, you get a line like `np_arr = tensor.cpu().numpy()`."
      ],
      "metadata": {
        "id": "u6mugSmG6EbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Operations\n",
        "\n",
        "Most operations that exist in numpy, also exist in PyTorch. A full list of operations can be found in the [PyTorch documentation](https://pytorch.org/docs/stable/tensors.html#), but we will review the most important ones here.\n",
        "\n",
        "The simplest operation is to add two tensors:"
      ],
      "metadata": {
        "id": "SnXURBoc6rF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = torch.rand(2, 3)\n",
        "x2 = torch.rand(2, 3)\n",
        "y = x1 + x2\n",
        "\n",
        "print(\"X1\", x1)\n",
        "print(\"X2\", x2)\n",
        "print(\"Y\", y)"
      ],
      "metadata": {
        "id": "9WJU5nd06s8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calling `x1 + x2` creates a new tensor containing the sum of the two inputs. However, we can also use in-place operations that are applied directly on the memory of a tensor. We therefore change the values of `x2` without the chance to re-accessing the values of `x2` before the operation. An example is shown below:"
      ],
      "metadata": {
        "id": "lneStW7N6vXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = torch.rand(2, 3)\n",
        "x2 = torch.rand(2, 3)\n",
        "print(\"X1 (before)\", x1)\n",
        "print(\"X2 (before)\", x2)\n",
        "\n",
        "x2.add_(x1)\n",
        "print(\"X1 (after)\", x1)\n",
        "print(\"X2 (after)\", x2)"
      ],
      "metadata": {
        "id": "b1IhhW8-6v5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In-place operations are usually marked with a underscore postfix (e.g. \"add_\" instead of \"add\").\n",
        "\n",
        "Another common operation aims at changing the shape of a tensor. A tensor of size (2,3) can be re-organized to any other shape with the same number of elements (e.g. a tensor of size (6), or (3,2), ...). In PyTorch, this operation is called `view`:"
      ],
      "metadata": {
        "id": "8DsRGAf66xPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = x1.view(2, 3)\n",
        "print(\"X\", x)"
      ],
      "metadata": {
        "id": "UNSy13w36zlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other commonly used operations include matrix multiplications, which are essential for neural networks. Quite often, we have an input vector $\\mathbf{x}$, which is transformed using a learned weight matrix $\\mathbf{W}$. There are multiple ways and functions to perform matrix multiplication, some of which we list below:\n",
        "\n",
        "* `torch.matmul`: Performs the matrix product over two tensors, where the specific behavior depends on the dimensions. If both inputs are matrices (2-dimensional tensors), it performs the standard matrix product. For higher dimensional inputs, the function supports broadcasting (for details see the [documentation](https://pytorch.org/docs/stable/generated/torch.matmul.html?highlight=matmul#torch.matmul)). Can also be written as `a @ b`, similar to numpy. \n",
        "* `torch.mm`: Performs the matrix product over two matrices, but doesn't support broadcasting (see [documentation](https://pytorch.org/docs/stable/generated/torch.mm.html?highlight=torch%20mm#torch.mm))\n",
        "* `torch.bmm`: Performs the matrix product with a support batch dimension. If the first tensor $T$ is of shape ($b\\times n\\times m$), and the second tensor $R$ ($b\\times m\\times p$), the output $O$ is of shape ($b\\times n\\times p$), and has been calculated by performing $b$ matrix multiplications of the submatrices of $T$ and $R$: $O_i = T_i @ R_i$\n",
        "* `torch.einsum`: Performs matrix multiplications and more (i.e. sums of products) using the Einstein summation convention. Explanation of the Einstein sum can be found in assignment 1.\n",
        "\n",
        "Usually, we use `torch.matmul` or `torch.bmm`. We can try a matrix multiplication with `torch.matmul` below."
      ],
      "metadata": {
        "id": "Ap1COXF366up"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indexing\n",
        "\n",
        "We often have the situation where we need to select a part of a tensor. Indexing works just like in numpy, so let's try it:"
      ],
      "metadata": {
        "id": "l1AByjJr7DAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(12).view(3, 4)\n",
        "print(\"X\", x)"
      ],
      "metadata": {
        "id": "opxMGzd76R_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x[:, 1])   # Second column"
      ],
      "metadata": {
        "id": "B6fMmONV6Sfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x[0])      # First row"
      ],
      "metadata": {
        "id": "Km1uW31D7Hxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x[:2, -1]) # First two rows, last column"
      ],
      "metadata": {
        "id": "8IlM7GNz7I_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x[1:3, :]) # Middle two rows"
      ],
      "metadata": {
        "id": "TpBoQFWS7KQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Distributions\n",
        "\n",
        "Source: https://pytorch.org/docs/stable/distributions.html\n",
        "\n",
        "The distributions package contains parameterizable probability distributions and sampling functions. This allows the construction of stochastic computation graphs and stochastic gradient estimators for optimization. This package generally follows the design of the TensorFlow Distributions package.\n",
        "\n",
        "During this course we will require that the various probability distributions\n",
        "that will stand for our models have the following properties (generally satisfied by the distributions available in Pyro and PyTorch Distributions):\n",
        "\n",
        " - we can efficiently sample from each \n",
        "\n",
        " - we can efficiently compute the pointwise probability density\n",
        "\n",
        " - is differentiable w.r.t. the parameters \n",
        "\n",
        "During the course we will use a lot of probabilistic models, so it is extremely useful to know some tools to make our job easier. \n",
        "\n",
        "Note that during next Lab we will learn about Pyro (source: http://pyro.ai/examples/intro_long.html) which builds upon torch distributions. "
      ],
      "metadata": {
        "id": "HI0xe3P-7QfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1 Multivariate Normal\n",
        "\n",
        "a) import `torch` and print the version you are using\n",
        "\n",
        "b) define a seed\n",
        "\n",
        "c) define a multivariate normal distribution with $\\mu = [1, 2]$ and $\\Sigma = \\begin{bmatrix} 1 & 0 \\\\ 0 & 2\\end{bmatrix}$\n",
        "\n",
        "d) sample $n = 100$ i.i.d. observations from it\n",
        "\n",
        "e) Compute the probability and log-probability of the sample obtained in d) under the distribution obtained in c). \n",
        "\n",
        "f) Explain **why do we want to work with log-probs?** How is e) connected to the likelihood? What is the difference?"
      ],
      "metadata": {
        "id": "s9d8bCMD8tH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR SOLUTION ###"
      ],
      "metadata": {
        "id": "SBN6-iAl-7sP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generally, shapes of distributions may be confusing. A perfect guide to make your life easier: https://bochang.me/blog/posts/pytorch-distributions/\n",
        "\n",
        "Additionally: https://stats.stackexchange.com/questions/2641/what-is-the-difference-between-likelihood-and-probability"
      ],
      "metadata": {
        "id": "wXft6LjWxJ4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Likelihoods\n",
        "\n",
        "Likelihood function is a function of parameters $\\theta \\in \\Theta$ given by $p(D \\mid \\theta)$. \n",
        "\n",
        "> Intuition: The likelihood expresses how probable the observed data set is for different settings of the parameter $\\theta$. "
      ],
      "metadata": {
        "id": "-atuH-ctEtDS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Exercise 2 Likelihood of Bernoulli\n",
        "\n",
        "> Give an analytical formula for the likelihood function of Bernoulli distribution. Calculate the MLE. "
      ],
      "metadata": {
        "id": "-iVEpFdUpROV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Coin Tossing\n",
        "\n",
        "a) Set $n$ to 1000\n",
        "\n",
        "b) Simulate $n$ fair coin tosses\n",
        "\n",
        "c) Simulate $n$ unfair coin tosses, with a probability of heads equal to $\\theta = 0.75$\n",
        "\n",
        "d) Compute the number of heads\n",
        "\n",
        "e) Compute the mean of your *data*\n",
        "\n",
        "\n",
        "f) Plot the likelihood function for the data from Exercise 2 as a function of parameter. \n",
        "\n",
        "g) Mark the maximum likelihood estimate of this likelihood on the plot.\n"
      ],
      "metadata": {
        "id": "RTowl6VSAAZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution"
      ],
      "metadata": {
        "id": "gqcIa3QUNta6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Your Solution ###"
      ],
      "metadata": {
        "id": "iT5zhJrKEuc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4: Prior, posterior and coin tossing (part 2)\n",
        "\n",
        "\n",
        "a) Add a plot the Beta distribution (prior for the binomial distribution) with parameters Î± = Î² = 3. What does this prior imply?\n",
        "\n",
        "b) Add a plot of the empirically calculated posterior distribution, computed as likelihood * prior (ignoring the normalization by the probability of the data)\n",
        "\n",
        "c) Add a plot of the theoretical posterior (reminder: Beta is conjugate to Binomial)\n",
        "\n",
        "d) Add a vertical line at the value maximizing the posterior and a legend"
      ],
      "metadata": {
        "id": "a-y_gVgip7aO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Your Solution ###"
      ],
      "metadata": {
        "id": "ww6x3keRcvVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 5 MLE and MAP\n",
        "\n",
        "Compute the difference between the two estimates for different as a function of the number of coin tosses $n$?\n",
        "\n",
        "What is happening? Why? Justify that the prior is often just a regularizer, by finding relevant examples.\n",
        "\n"
      ],
      "metadata": {
        "id": "gMvKY50FeALQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR SOLUTION ###"
      ],
      "metadata": {
        "id": "5z-uHq-oSFRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Motivation for using Pyro\n",
        "\n",
        "Next week we will start using Pyro. A good introductory tutorial is provided [here](https://pyro.ai/examples/intro_long.html#Models-in-Pyro). Please cover the section Models in Pyro. \n",
        "\n",
        "Often, models we work with don't have an analytical form of the likelihood we want to maximize, no problem!"
      ],
      "metadata": {
        "id": "vxTelXtqepcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 install pyro-ppl \n",
        "import pyro\n",
        "from pyro.distributions import constraints\n",
        "\n",
        "def model_mle(data):\n",
        "    f = pyro.param(\"head_probs\", torch.tensor(0.5),\n",
        "                   constraint=constraints.unit_interval)\n",
        "    with pyro.plate(\"data\", data.size(0)):\n",
        "        pyro.sample(\"obs\", pyro.distributions.Bernoulli(f), obs=data)"
      ],
      "metadata": {
        "id": "U1w165gVdx9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "pyro.render_model(model_mle, model_args=(biased_coin_tosses,), render_distributions=True, render_params=True)"
      ],
      "metadata": {
        "id": "DfD6_TjSd1he",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, guide, data, lr=0.005, n_steps=201):\n",
        "    pyro.clear_param_store()\n",
        "    adam_params = {\"lr\": lr}\n",
        "    adam = pyro.optim.Adam(adam_params)\n",
        "    svi = SVI(model, guide, adam, loss=Trace_ELBO())\n",
        "\n",
        "    for step in range(n_steps):\n",
        "        loss = svi.step(data)\n",
        "        if step % 50 == 0:\n",
        "            print('[iter {}]  loss: {:.4f}'.format(step, loss))\n",
        "\n",
        "\n",
        "def guide_mle(data):\n",
        "    pass\n",
        "\n"
      ],
      "metadata": {
        "id": "uEIsOoArbgfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For biased coin toss\n",
        "train(model_mle, guide_mle, biased_coin_tosses)\n",
        "\n",
        "mle_estimate = pyro.param(\"head_probs\").item()\n",
        "print(\"Our MLE estimate of the probability of getting a head is {:.3f}\".format(mle_estimate))"
      ],
      "metadata": {
        "id": "9x411Te8uQ59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For fair coin toss\n",
        "train(model_mle, guide_mle, fair_coin_tosses)\n",
        "\n",
        "mle_estimate = pyro.param(\"head_probs\").item()\n",
        "print(\"Our MLE estimate of the probability of getting a head is {:.3f}\".format(mle_estimate))"
      ],
      "metadata": {
        "id": "hJMZ9cRzuJRO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}