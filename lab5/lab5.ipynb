{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SAD2 2022/2023 - Introduction to Variational Inference\n",
    "\n",
    "This week we will introduce the variational inference. The concepts discussed in previous week, namely MLE, ELBO, and KL divergence come together in this framework. Variational inference embraces a family of methods for approximating complicated densities by a simpler class of densities, as introduced [here](https://www.cs.toronto.edu/~bonner/courses/2020s/csc2547/week2/variational-inference-tutorial,-mohamed,-2015.pdf).\n",
    "\n",
    "This week we will start working with Pyro. Pyro is a dynamic object-oriented programming language created to support what we call the probabilistic programming. Pyro combines the elegance and expressiveness of Python with the performance and robustness of C++. Pyro is built on top of the PyTorch library and uses the same underlying C++ code.\n",
    "\n",
    "This lab is heavily based on [Pyro tutorial on Stochastic Variational Inference](https://pyro.ai/examples/svi_part_i.html). It is possible to implement this exercise solely based on this tutorial, however, we want to let you know that there are more useful resources regarding Pyro (cue Project 1 ;)):\n",
    "\n",
    "   - [Introduction to Pyro](http://pyro.ai/examples/intro_long.html)\n",
    "   - [GMM in Pyro](https://pyro.ai/examples/gmm.html)\n",
    "   - [SVI Part II: Conditional Independence, Subsampling, and Amortization](https://pyro.ai/examples/svi_part_ii.html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PLOTTING\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. The \"Variational\" in \"Variational Inference\"\n",
    "\n",
    "The \"Variational\" in \"Variational Inference\" is a reference to the fact that in variational inference we optimize a function, called the variational lower bound, over a set of variables known as variational parameters. The variational parameters are usually chosen to be the natural parameters of a probability distribution, which means that they can be used to define a probability distribution. The goal of variational inference is to find a set of variational parameters that makes the variational lower bound as tight as possible.\n",
    "\n",
    "There are many different ways to formulate the variational lower bound, but the most common one is based on the Kullback-Leibler (KL) divergence. The KL divergence is a measure of the difference between two probability distributions. In the context of variational inference, we can think of the KL divergence as a measure of the difference between the true posterior distribution and the approximate posterior distribution. The goal of variational inference is to find a set of variational parameters that minimizes the KL divergence between the true posterior and the approximate posterior.\n",
    "\n",
    "Recall the KL divergence is defined as:\n",
    "\n",
    "$KL(p||q) = \\sum_x p(x) \\log \\frac{p(x)}{q(x)}$\n",
    "\n",
    "where $p$ and $q$ are two probability distributions. In the context of variational inference, $p$ is the true posterior distribution and $q$ is the approximate posterior distribution.\n",
    "\n",
    "The variational lower bound is then defined as:\n",
    "\n",
    "$\\mathcal{L}(p, q) = \\sum_x p(x) \\log q(x) - KL(p||q)$\n",
    "\n",
    "The goal of variational inference is to find a set of variational parameters that minimizes the KL divergence between the true posterior and the approximate posterior. In other words, we want to find a set of variational parameters that makes the variational lower bound as tight as possible.\n",
    "\n",
    "There are many different ways to optimize the variational lower bound. The most common one is gradient descent. In gradient descent, we take the gradient of the variational lower bound with respect to the variational parameters and then update the variational parameters in the direction that maximizes the variational lower bound.\n",
    "\n",
    "There are many different ways to initialize the variational parameters. The most common one is to initialize them randomly. Once the variational parameters have been optimized, we can then use them to define a probability distribution. This probability distribution is our approximate posterior distribution. We can then use this approximate posterior distribution to make predictions about the data.\n",
    "\n",
    "There are many different ways to evaluate the quality of the approximate posterior distribution as well. The most common one is to compute the KL divergence between the true posterior and the approximate posterior. Other common evaluation metrics include the log-likelihood and the predictive accuracy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. ELBO and KL divergence in Variational Inference [TODO]\n",
    "Same question, differently formulated: Explain VI using only (mostly) ELBO and KL divergence\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Infer the posterior distribution of $\\mu$ and $\\sigma$ ($\\tau$) in the example form the lecture."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem statement"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.imshow(plt.imread('img.png'))\n",
    "\n",
    "# Hide grid lines\n",
    "ax.grid(False)\n",
    "\n",
    "# Hide axes ticks\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Draw data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.distributions import Normal\n",
    "\n",
    "MEAN = 5.0\n",
    "STD = 0.5\n",
    "N_SAMPLES = 20\n",
    "\n",
    "#smoll data\n",
    "data = torch.tensor([10., 10., 10., 10., 10.])\n",
    "\n",
    "# #big data\n",
    "# NORMAL = Normal(torch.tensor([MEAN]), torch.tensor([STD]))\n",
    "# data = NORMAL.sample((N_SAMPLES,))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define model\n",
    "See [Models in Pyro](http://pyro.ai/examples/svi_part_i.html) section Simple Example. **Don't** use plate here, we will do it at the very end!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "\n",
    "def model(data):\n",
    "    ### PLS, WRITE ME ###"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sudo apt-get install graphviz\n",
    "# pip install graphviz\n",
    "\n",
    "pyro.render_model(model, model_args=(data,), render_distributions=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define guide $q_{\\phi}({\\bf z})$\n",
    "\n",
    "See [Guides in Pyro](http://pyro.ai/examples/intro_long.html#:~:text=original%20KL%2Ddivergence.-,Background%3A%20%E2%80%9Cguide%E2%80%9D%20programs%20as%20flexible%20approximate%20posteriors,%C2%B6,-In%20variational%20inference)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def guide(data):\n",
    "    ### PLS, WRITE ME ###\n",
    "\n",
    "    return {\"alpha_q\": alpha_q, \"beta_q\": beta_q, \"mu_q\": mu_q, \"lambda_q\": lambda_q, \"mean\": mean, \"std\": std}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pyro.render_model(guide, model_args=(data,), render_distributions=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define optimizer\n",
    "\n",
    "Optional: do some tweaks to the optimizer and see if it changes the params and convergence."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyro.optim import Adam\n",
    "\n",
    "adam_params = {\"lr\": 0.005}\n",
    "optimizer = Adam(adam_params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define [$SVI$ class](https://docs.pyro.ai/en/stable/inference_algos.html?highlight=svi)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyro.infer import SVI, Trace_ELBO\n",
    "\n",
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Do the inference; report the inferred mean and std"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_steps = 500\n",
    "\n",
    "# do gradient steps\n",
    "for step in range(n_steps):\n",
    "    svi.step(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Report your parameters here"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot convergence (loss + mean wrt step)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    ### WOW, WOW, SUCH CODE ###"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pyro.render_model(model, model_args=(data,), render_distributions=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}