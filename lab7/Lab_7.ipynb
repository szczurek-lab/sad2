{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Variational Autoencoders Continued\n",
        "\n",
        "Use your implementation of a VAE from last week to complete the following task. \n",
        "\n",
        "\n",
        "### Task 1\n",
        "\n",
        "Choose the right latent size. Fit a PCA to your latent space. See how much variance is explained by the dimensions of your latent space and choose the latent size accordingly. Compare -ELBO of VAE trained with different latent dimensions. Do you see any changes? Visualize the data in the latent space using PCA. Plot the data together with class information. Do you see any clustering of the data? \n",
        "\n",
        "### Task 2\n",
        "\n",
        "Use the sample method to sample points in the latent space and then decode them. Do they produce good quality data points? Encode the sampled data points. What is the average L2 distance between the original points in the latent space and the decoded and encoded points?\n",
        "\n",
        "### Task 3\n",
        "\n",
        "Break down -ELBO into reconstruction error and the KL divergence term. Plot both quantities on convergence plots like in Task 1 from last week. Do you see any patterns? \n",
        "\n",
        "\n",
        "### Task 4\n",
        "\n",
        "Train your VAE with different $\\beta$ regularization coefficients. Try at least $\\beta = 0, 0.1, 1, 10$. How does it changes the learning curve? Plot $-ELBO$, reconstruction and regularization losses and visualize reconstructed sample points and the latent space. What do you see in sample quality? What do you see in latent organization? Do you observe some kind of trade-off?\n",
        "\n",
        "(Optional)Try training with only the KL divergence. \n",
        "\n",
        "### Task 5 (Optional)\n",
        "\n",
        "Compare training with MSE and log prob. Can you derive an analytical relationship between the two?\n",
        "\n",
        "### Task 6 (Optional)\n",
        "\n",
        "Implement a continuous bernoulli decoder. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NnAR1LAYShYZ"
      }
    }
  ]
}