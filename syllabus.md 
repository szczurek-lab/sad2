# Syllabus

1. Frequentist vs Bayesian approach in statistical modeling
2. Bayesian networks (probabilistic graphical models)
3. Hidden Markov Models and Gaussian Mixture Models
4. EM algorithm (parameter estimation in models with hidden variables)
5. Variational inference
6. Exact inference in graphical models
   - Factor graphs,
   - The sum product algorithm,
   - Cluster trees, 
   - Potentials, 
   - Message passing, 
   - Junction tree algorithm.

7. PyClone: real-life example of a Bayesian graphical model
8. Sampling methods
   - MCMC
   - Gibbs sampling

9. Model selection 
   - Model evidence,
   - Learning model structure, 
   - Tree models, 
   - General models, 
   - Structural EM

---

- Parameter inference in probabilistic graphical models with fully observed data 
- Markov chains and Hidden Markov 
  Models, as examples of Bayesian networks, parameter estimation and inference

---

## Project 1: Bayesian Networks
Learning Bayesian networks from gene expression data

## Project 2: Gibbs Sampling 

--- 

## Assessment 

### Scoring:

- 50% exam at the end (a test)
- 15% computational project 1 
- 15% computational project 2 
- Mid-term test 15% 
- 5% lab activity

### Required to pass: 50%

### Zero exam
Oral, the date is agreed individually, no later than a week before the final exam.
Criteria for admission to the zero exam: 45 points for projects and test.

